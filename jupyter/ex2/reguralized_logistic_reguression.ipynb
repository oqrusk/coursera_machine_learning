{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "import pandas as pd\n",
    "from scipy import special, optimize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.arange(0, 40, 10)\n",
    "# print(a.shape)\n",
    "# print(a)\n",
    "# k = np.ones((a.shape[0], ))\n",
    "\n",
    "# b= np.stack((a, k), axis=-1)\n",
    "# print(b.shape)\n",
    "# print(b)\n",
    "\n",
    "# c = np.append(a, k)\n",
    "# print(c.shape)\n",
    "# print(c)\n",
    "\n",
    "# x = a[:, np.newaxis]\n",
    "# y = np.ones((a.shape[0], 1))\n",
    "# d = np.concatenate((y, x), axis=1)\n",
    "# print(d.shape)\n",
    "# print(d)\n",
    "\n",
    "\n",
    "# # ベクトルと　行列の扱いが違うのがメンドイ。\n",
    "# outa = np.ones((X.shape[0], ))\n",
    "# print(outa[:4])\n",
    "\n",
    "# print('X[:, 0]:shape', X[:, 0].shape)\n",
    "# out = np.ones((X.shape[0], 1))\n",
    "\n",
    "# print(out[:4])\n",
    "# # print('out:shape', out.shape)\n",
    "# # np.hsplit(X, 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Online Class - Exercise 2: Logistic Regression\n",
    "\n",
    "Instructions\n",
    "\n",
    "\n",
    "This file contains code that helps you get started on the second part of the exercise which covers regularization with logistic regression.\n",
    "\n",
    "You will need to complete the following functions in this exericse:\n",
    "\n",
    "- sigmoid.m\n",
    "- costFunction.m\n",
    "- predict.m\n",
    "- costFunctionReg.m\n",
    "\n",
    "For this exercise, you will not need to change any code in this file, or any other files other than those mentioned above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "The first two columns contains the X values and the third column contains the label (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score1</th>\n",
       "      <th>score2</th>\n",
       "      <th>admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.68494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.69225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.50219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.46564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     score1   score2  admitted\n",
       "0  0.051267  0.69956         1\n",
       "1 -0.092742  0.68494         1\n",
       "2 -0.213710  0.69225         1\n",
       "3 -0.375000  0.50219         1\n",
       "4 -0.513250  0.46564         1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./ex2data2.txt\", names=['score1', 'score2', 'admitted'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.as_matrix([\"score1\",\"score2\"])\n",
    "y = df.as_matrix([\"admitted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:shape (118, 2)\n",
      "y:shape (118, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X:shape', X.shape)\n",
    "print('y:shape', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noTScO9R7H8QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x151056ca58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_data(X, y):\n",
    "    pos = X[(y==1)[:, 0]]\n",
    "    neg = X[(y==0)[:,0]]\n",
    "\n",
    "    plt.plot(pos[:, 0], pos[:, 1], \"o\", label=u'y = 1')\n",
    "    plt.plot(neg[:, 0], neg[:, 1], \"x\", label=u'y = 0')\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.legend()\n",
    "\n",
    "# call this function\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Regularized Logistic Regression\n",
    "\n",
    "In this part, you are given a dataset with data points that are not linearly separable. \n",
    "However, you would still like to use logistic regression to classify the data points.\n",
    "\n",
    "To do so, you introduce more features to use -- in particular, you add polynomial features to our data matrix (similar to polynomial regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_feature(X1, X2):\n",
    "    \n",
    "    degree = 6\n",
    "    out = np.ones((X1.shape[0], 1))\n",
    "\n",
    "    for i in range(1, degree):\n",
    "        for j in  range(0, i):\n",
    "            n = np.power(X1, i - j) * np.power(X2, j)\n",
    "            out = np.hstack((out, n))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_theta:  (16, 1)\n",
      "X:  (118, 16)\n",
      "hx shape (118, 1)\n",
      "y shape (118, 1)\n",
      "regular shape (1, 1)\n",
      "theta shape (16, 1)\n",
      "J shape (1, 1)\n",
      "grad shape (16, 1)\n",
      "Cost at initial theta (zeros): [[ 0.69314718]]\n",
      "Expected cost (approx): 0.693\n",
      "Gradient at initial theta (zeros) - first five values only:\n",
      "[[ 0.00847458]\n",
      " [ 0.01878809]\n",
      " [ 0.05034464]\n",
      " [ 0.01150133]\n",
      " [ 0.01835599]]\n",
      "Expected gradients (approx) - first five values only:\n",
      "0.0085\n",
      " 0.0188\n",
      " 0.0001\n",
      " 0.0503\n",
      " 0.0115\n"
     ]
    }
   ],
   "source": [
    "def cost_function_reg(theta, X, y, lmd):\n",
    "    '''\n",
    "    function [J, grad] = costFunctionReg(theta, X, y, lambda)\n",
    "    COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization\n",
    "    J = COSTFUNCTIONREG(theta, X, y, lambda) \n",
    "    computes the cost of using theta as the parameter for regularized logistic regression and the gradient of the cost w.r.t. to the parameters.\n",
    "    '''\n",
    "    \n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    '''\n",
    "    ====================== YOUR CODE HERE ======================\n",
    "    Instructions: Compute the cost of a particular choice of theta.\n",
    "                           You should set J to the cost.\n",
    "                           Compute the partial derivatives and set grad to the partial derivatives of the cost w.r.t. each parameter in theta\n",
    "    '''\n",
    "    \n",
    "    if len(theta.shape) == 1:\n",
    "        theta = theta[:, np.newaxis]\n",
    "    \n",
    "    hx = sigmoid(np.dot(X, theta))\n",
    "       \n",
    "    theta[0, :] = 0\n",
    "    regular = lmd / (2*m) * np.dot(theta.T, theta)\n",
    "    \n",
    "#     '''\n",
    "    print('init_theta: ', theta.shape)\n",
    "    print('X: ', X.shape)\n",
    "    print(\"hx shape\", hx.shape)\n",
    "    print(\"y shape\", y.shape)\n",
    "    print(\"regular shape\", regular.shape)\n",
    "    print(\"theta shape\", theta.shape)\n",
    "#     '''\n",
    "    \n",
    "    J = (1/m) * ( - np.dot(y.T, np.log(hx)) - np.dot((1 - y).T, np.log(1 - hx))) + regular\n",
    "\n",
    "    \n",
    "    y_m = y# [:, np.newaxis] # y.shape => (m, ) h.shape =>(m,1)\n",
    "    grad = (1/m) * np.dot(X.T, (hx -  y_m))\n",
    "    \n",
    "    print(\"J shape\", J.shape)\n",
    "    print(\"grad shape\", grad.shape)\n",
    "    return (J, grad)\n",
    "\n",
    "\n",
    "# Add Polynomial Features\n",
    "# Note that mapFeature also adds a column of ones for us, so the intercept term is handled\n",
    "\n",
    "X_s = np.hsplit(X, 2)\n",
    "X = map_feature(X_s[0], X_s[1])\n",
    "\n",
    "# Initialize fitting parameters\n",
    "(m, n) = X.shape\n",
    "initial_theta = np.zeros((n, 1))\n",
    "\n",
    "# Set regularization parameter lambda to 1\n",
    "lmd = 1\n",
    "\n",
    "# Compute and display initial cost and gradient for regularized logistic regression\n",
    "res = cost_function_reg(initial_theta, X, y, lmd)\n",
    "cost = res[0]\n",
    "grad = res[1]\n",
    "\n",
    "print(\"Cost at initial theta (zeros): {0}\".format(cost))\n",
    "print(\"Expected cost (approx): 0.693\")\n",
    "print('Gradient at initial theta (zeros) - first five values only:')\n",
    "print(grad[:5])\n",
    "print(\"Expected gradients (approx) - first five values only:\")\n",
    "print(\"0.0085\\n 0.0188\\n 0.0001\\n 0.0503\\n 0.0115\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regularization and Accuracies\n",
    "\n",
    "Optional Exercise:\n",
    "In this part, you will get to try different values of lambda and see how regularization affects the decision coundart\n",
    "\n",
    "Try the following values of lambda (0, 1, 10, 100).\n",
    "\n",
    "How does the decision boundary change when you vary lambda? \n",
    "How does the training set accuracy vary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(theta, X, y):\n",
    "    plt.plot(pos[:, 0], pos[:, 1], \"o\", label=u'y = 1')\n",
    "    plt.plot(neg[:, 0], neg[:, 1], \"x\", label=u'y = 0')\n",
    "    \n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "\n",
    "#     plt.plot(, , \"-g\", label=u'Decision boundary')\n",
    "#     plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(theta_cv, X):\n",
    "    m = X.shape[0] # Number of training examples\n",
    "    # You need to return the following variables correctly\n",
    "    p = np.zeros((m, 1))\n",
    "    return sigmoid(np.dot(X, theta_cv)) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_theta:  (16, 1)\n",
      "init_theta:  (16, 1)\n",
      "X:  (118, 16)\n",
      "hx shape (118, 1)\n",
      "y shape (118, 1)\n",
      "regular shape (1, 1)\n",
      "theta shape (16, 1)\n",
      "J shape (1, 1)\n",
      "grad shape (16, 1)\n",
      "init_theta:  (16, 1)\n",
      "X:  (118, 16)\n",
      "hx shape (118, 1)\n",
      "y shape (118, 1)\n",
      "regular shape (1, 1)\n",
      "theta shape (16, 1)\n",
      "J shape (1, 1)\n",
      "grad shape (16, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (16,1) and (16,1) not aligned: 1 (dim 1) != 16 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-76c62c19b4ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcost_function_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BFGS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                      _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n\u001b[0;32m--> 934\u001b[0;31m                                           old_fval, old_old_fval, amin=1e-100, amax=1e100)\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_LineSearchError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;31m# Line search failed to find a better solution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_line_search_wolfe12\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[1;32m    763\u001b[0m     ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n\u001b[1;32m    764\u001b[0m                              \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m                              **kwargs)\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/linesearch.py\u001b[0m in \u001b[0;36mline_search_wolfe1\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mderphi0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgfk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     stp, fval, old_fval = scalar_search_wolfe1(\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (16,1) and (16,1) not aligned: 1 (dim 1) != 16 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Initialize fitting parameters\n",
    "(m, n) = X.shape\n",
    "\n",
    "initial_theta = np.zeros((n, 1))\n",
    "print('initial_theta: ', initial_theta.shape)\n",
    "# Set regularization parameter lambda to 1 (you should vary this)\n",
    "lmd = 1\n",
    "\n",
    "# Optimize\n",
    "f = lambda t: cost_function_reg(t, X, y, lmd)\n",
    "res = optimize.minimize(f, initial_theta, method='BFGS', jac=True, options={'maxiter': 400})\n",
    "\n",
    "theta = res.x\n",
    "\n",
    "# Plot Boundary\n",
    "plot_decision_boundary(theta, X, y)\n",
    "\n",
    "# Compute accuracy on our training set\n",
    "theta_cv = np.array(theta, ndmin=2).T \n",
    "p = predict(theta_cv, X) \n",
    "\n",
    "print('Train Accuracy: {0}'.format(np.mean(p == y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %whos\n",
    "# optimize.minimize\n",
    "tht= (1, 2)\n",
    "len(tht)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
